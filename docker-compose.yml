# RecallHub - Full Stack Docker Compose
# All services run on ports 11000-11100
#
# Port Assignments:
#   - MongoDB:  11017
#   - Backend:  11000
#   - Frontend: 11080
#   - Airbyte Webapp: 11020 (optional, via docker-compose.airbyte.yml)
#   - Airbyte API:    11021 (optional, via docker-compose.airbyte.yml)
#   - Airbyte Builder: 11022 (optional, via docker-compose.airbyte.yml)
#
# To run with Airbyte:
#   docker-compose -f docker-compose.yml -f docker-compose.airbyte.yml up -d

services:
  # ===========================================
  # MongoDB Atlas Local - Vector Search & Atlas Search
  # ===========================================
  mongodb:
    image: mongodb/mongodb-atlas-local:8.0
    container_name: rag-mongodb
    hostname: rag-mongodb-host
    ports:
      - "11017:27017"
    volumes:
      - ./data/mongoDB/db:/data/db
      - ./data/mongoDB/configdb:/data/configdb
      - ./scripts:/docker-entrypoint-initdb.d:ro
    environment:
      - DO_NOT_TRACK=1
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - rag-network
    restart: unless-stopped

  # ===========================================
  # Backend - FastAPI REST API
  # ===========================================
  # Build: Uses shared base image for fast rebuilds
  #   1. First time: docker build -f backend/Dockerfile.base -t recallhub-backend-base:latest .
  #   2. Code changes: docker-compose build backend (uses Dockerfile.fast, ~30 sec)
  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile.fast
    container_name: rag-backend
    ports:
      - "11000:8000"
    depends_on:
      mongodb:
        condition: service_healthy
    environment:
      # MongoDB connection (internal container network)
      - MONGODB_URI=mongodb://mongodb:27017/?directConnection=true
      - MONGODB_DATABASE=${MONGODB_DATABASE:-rag_db}
      
      # LLM Configuration
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - LLM_API_KEY=${LLM_API_KEY}
      - LLM_MODEL=${LLM_MODEL:-gpt-4.1-mini}
      - LLM_BASE_URL=${LLM_BASE_URL:-https://api.openai.com/v1}
      
      # Embedding Configuration
      - EMBEDDING_PROVIDER=${EMBEDDING_PROVIDER:-openai}
      - EMBEDDING_API_KEY=${EMBEDDING_API_KEY}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-text-embedding-3-small}
      - EMBEDDING_BASE_URL=${EMBEDDING_BASE_URL:-https://api.openai.com/v1}
      - EMBEDDING_DIMENSION=${EMBEDDING_DIMENSION:-1536}
      
      # Profile configuration
      - PROFILES_PATH=/app/profiles.yaml
      # ACTIVE_PROFILE can be set to override profiles.yaml, but defaults to saved value
      
      # Airbyte Integration (optional - for Confluence/Jira)
      - AIRBYTE_ENABLED=${AIRBYTE_ENABLED:-false}
      - AIRBYTE_API_URL=${AIRBYTE_API_URL:-http://airbyte-server:8001}
      - AIRBYTE_WEBAPP_URL=${AIRBYTE_WEBAPP_URL:-http://localhost:11020}
    volumes:
      - ./documents:/app/documents
      - ./projects:/app/projects
      - ./profiles.yaml:/app/profiles.yaml
      # Profile-specific document folders (create these on host to use)
      - ./mounts/parhelion-energy:/app/mounts/parhelion-energy:ro
      - ./mounts/gdrive-root:/app/mounts/gdrive-root:ro
    networks:
      - rag-network
    restart: unless-stopped

  # ===========================================
  # Ingestion Worker - Background document processing
  # ===========================================
  # Build: Uses same shared base image as backend
  #   Code changes: docker-compose build ingestion-worker (uses Dockerfile.worker, ~30 sec)
  ingestion-worker:
    build:
      context: .
      dockerfile: backend/Dockerfile.worker
    container_name: rag-ingestion-worker
    depends_on:
      mongodb:
        condition: service_healthy
    environment:
      # MongoDB connection (internal container network)
      - MONGODB_URI=mongodb://mongodb:27017/?directConnection=true
      - MONGODB_DATABASE=${MONGODB_DATABASE:-rag_db}
      
      # Embedding Configuration (needed for document processing)
      - EMBEDDING_PROVIDER=${EMBEDDING_PROVIDER:-openai}
      - EMBEDDING_API_KEY=${EMBEDDING_API_KEY}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-text-embedding-3-small}
      - EMBEDDING_BASE_URL=${EMBEDDING_BASE_URL:-https://api.openai.com/v1}
      - EMBEDDING_DIMENSION=${EMBEDDING_DIMENSION:-1536}
      
      # LLM Configuration (for audio transcription via Whisper)
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - LLM_API_KEY=${LLM_API_KEY}
      - LLM_BASE_URL=${LLM_BASE_URL:-https://api.openai.com/v1}
      
      # Profile configuration
      - PROFILES_PATH=/app/profiles.yaml
    volumes:
      - ./documents:/app/documents:ro
      - ./projects:/app/projects:ro
      - ./profiles.yaml:/app/profiles.yaml:ro
      # Profile-specific document folders (create these on host to use)
      - ./mounts/parhelion-energy:/app/mounts/parhelion-energy:ro
      - ./mounts/gdrive-root:/app/mounts/gdrive-root:ro
    networks:
      - rag-network
    restart: unless-stopped

  # ===========================================
  # Frontend - React + Vite (served by nginx)
  # ===========================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: rag-frontend
    ports:
      - "11080:80"
    depends_on:
      - backend
    networks:
      - rag-network
    restart: unless-stopped

  # ===========================================
  # CLI Agent (optional - for terminal access)
  # ===========================================
  cli:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rag-cli
    profiles:
      - cli
    depends_on:
      mongodb:
        condition: service_healthy
    environment:
      - MONGODB_URI=mongodb://mongodb:27017/?directConnection=true
      - MONGODB_DATABASE=${MONGODB_DATABASE:-rag_db}
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - LLM_API_KEY=${LLM_API_KEY}
      - LLM_MODEL=${LLM_MODEL:-gpt-4.1-mini}
      - LLM_BASE_URL=${LLM_BASE_URL:-https://api.openai.com/v1}
      - EMBEDDING_PROVIDER=${EMBEDDING_PROVIDER:-openai}
      - EMBEDDING_API_KEY=${EMBEDDING_API_KEY}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-text-embedding-3-small}
      - EMBEDDING_BASE_URL=${EMBEDDING_BASE_URL:-https://api.openai.com/v1}
      - EMBEDDING_DIMENSION=${EMBEDDING_DIMENSION:-1536}
      - PROFILES_PATH=/app/profiles.yaml
      # ACTIVE_PROFILE can be set to override profiles.yaml
    volumes:
      - ./documents:/app/documents
      - ./projects:/app/projects
      - ./profiles.yaml:/app/profiles.yaml
    stdin_open: true
    tty: true
    networks:
      - rag-network

networks:
  rag-network:
    driver: bridge
